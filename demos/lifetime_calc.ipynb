{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Life time Calculation \n",
    "\n",
    "This Notebook shows a general calculation stream for a nominal and local stress reliability approach.\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Stress derivation\n",
    "We are starting with the imported rainflow matrices. More information about the time series and loading handlin and the RF generation you can find in the notebook [time_series_handling](time_series_handling.ipynb)\n",
    "\n",
    "1. Mean stress correction\n",
    "2. Multiplication with repeating factor of every manoveur\n",
    "\n",
    "### Damage Calculation\n",
    "1. Select the damage calculation method (Miner elementary, Miner-Haibach, ...)\n",
    "2. Calculate the damage for every load level and the damage sum\n",
    "3. Calculate the failure probability with or w/o field scatter\n",
    "\n",
    "### Local stress approach\n",
    "1. Load the FE mesh\n",
    "2. Apply the load history to the FE mesh\n",
    "3. Calculate the damage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pylife.utils.histogram import *\n",
    "import pylife.stress.timesignal as ts\n",
    "\n",
    "import pylife.stress.equistress\n",
    "\n",
    "import pylife.stress\n",
    "import pylife.strength.meanstress as MS\n",
    "import pylife.strength.fatigue\n",
    "\n",
    "import pylife.mesh.meshsignal\n",
    "\n",
    "from pylife.strength import failure_probability as fp\n",
    "import pylife.vmap\n",
    "\n",
    "import pyvista as pv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "from helper_functions import plot_rf\n",
    "# mpl.style.use('seaborn')\n",
    "# mpl.style.use('seaborn-notebook')\n",
    "mpl.style.use('bmh')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r rf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meanstress transformation\n",
    "\n",
    "Here we are using the *FKM Goodman* approach to calculate the meanstress transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanstress_sensitivity = pd.Series({\n",
    "    'M': 0.3,\n",
    "    'M2': 0.2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dict = {k: rf_act.meanstress_transform.fkm_goodman(meanstress_sensitivity, R_goal=-1.).to_pandas() for k, rf_act in rf_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeating factor\n",
    "If you want to apply a repeating factor to your loads you can do it very easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating = {\n",
    "    'wn': 50.0, \n",
    "    'sine': 25.0,\n",
    "    'SoR': 25\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dict = {k: transformed_dict[k] * repeating[k] for k in repeating.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are calculating a seperat load case, where we summarize the three channels together. Later on we can compare the damage results of this channel with the sum of the other channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dict['total'] = pd.concat([load_dict[k] for k in load_dict.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.interval_range(0., load_dict['total'].load_collective.use_class_right().amplitude.max(), 64)\n",
    "rebinned_dict = {k: rebin_histogram(v.load_collective.amplitude_histogram, bins) for k, v in load_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(10, 5))\n",
    "\n",
    "for k, v in rebinned_dict.items():\n",
    "    amplitude = v.index.right[::-1]\n",
    "    cycles = v[::-1]\n",
    "    ax[0].step(cycles, amplitude, label=k)\n",
    "    ax[1].step(np.cumsum(cycles), amplitude, label=k)\n",
    "\n",
    "for title, ai in zip(['Count', 'Cumulated'], ax):\n",
    "    ai.set_title(title)\n",
    "    ai.xaxis.grid(True)\n",
    "    ai.legend()\n",
    "    ai.set_xlabel('count')\n",
    "    ai.set_ylabel('amplitude')\n",
    "    ai.set_ylim((0,max(amplitude)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nominal stress approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Material parameters\n",
    "You can create your own material data from Woeler tests using the Notebook [woehler_analyzer](woehler_analyzer.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_1 = 8\n",
    "mat = pd.Series({\n",
    "    'k_1': k_1,\n",
    "    'k_2' : 2 * k_1 - 1,\n",
    "    'ND': 1.0e6,\n",
    "    'SD': 300.0,\n",
    "    'TN': 12.,\n",
    "    'TS': 1.1\n",
    "})\n",
    "display(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Damage Calculation\n",
    "Now we can calculate the damage for every loadstep and summarize this damage to get the total damage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# damage for every load range\n",
    "damage_miner_original = {k: mat.fatigue.damage(v.load_collective) for k, v in load_dict.items()}\n",
    "damage_miner_elementary = {k: mat.fatigue.miner_elementary().damage(v.load_collective) for k, v in load_dict.items()}\n",
    "damage_miner_haibach = {k: mat.fatigue.miner_haibach().damage(v.load_collective) for k, v in load_dict.items()}\n",
    "\n",
    "# and the damage sum\n",
    "damage_sum_miner_haibach = {k: v.sum() for k, v in damage_miner_haibach.items()}\n",
    "# ... and so on\n",
    "print(damage_sum_miner_haibach)\n",
    "\n",
    "print(\"total from sum: \" + str(damage_sum_miner_haibach[\"wn\"] + damage_sum_miner_haibach[\"sine\"] + damage_sum_miner_haibach[\"SoR\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the sum of the first three load channels with the 'total' one. The different is based on the fact that we have used 10 bins only. Try to rerun the notebook with a higher bin resolution and you will see the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### Plot the damage vs collectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = mat.woehler\n",
    "cyc = pd.Series(np.logspace(1, 12, 200))\n",
    "for pf, style in zip([0.1, 0.5, 0.9], ['--', '-', '--']):\n",
    "    load = wc.basquin_load(cyc, failure_probability=pf)\n",
    "    plt.plot(cyc, load, style)\n",
    "\n",
    "plt.step(np.cumsum(rebinned_dict['total'][::-1]), rebinned_dict['total'].index.right[::-1])\n",
    "plt.xlabel(\"cylces\"), plt.ylabel(\"amplitude\")\n",
    "plt.loglog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without field scatter\n",
    "\n",
    "In the first use case we assume, that we have the material scatter only. With that we can calculate the failure probability using the *FailureProbability* class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D50 = 0.01\n",
    "\n",
    "damage = damage_sum_miner_haibach[\"total\"]\n",
    "\n",
    "di = np.logspace(np.log10(1e-1*damage), np.log10(1e2*damage), 1000)\n",
    "std = pylife.utils.functions.scattering_range_to_std(mat.TN)\n",
    "failprob = fp.FailureProbability(D50, std).pf_simple_load(di)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(di, failprob, label='cdf')\n",
    "plt.vlines(damage, ymin=0, ymax=1, color=\"black\")\n",
    "plt.xlabel(\"Damage\")\n",
    "plt.ylabel(\"cdf\")\n",
    "plt.title(\"Failure probability = %.2e\" %fp.FailureProbability(D50,std).pf_simple_load(damage))  \n",
    "plt.ylim(0,max(failprob))\n",
    "plt.xlim(min(di), max(di))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With field scatter ###\n",
    "If we have the field scatter we can calculate the failure probability using convoluation of the probility density functions of the load and the strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_std = 0.35\n",
    "fig, ax = plt.subplots()\n",
    "# plot pdf of material\n",
    "mat_pdf = norm.pdf(np.log10(di), loc=np.log10(D50), scale=std)\n",
    "ax.semilogx(di, mat_pdf, label='pdf_mat')\n",
    "# plot pdf of load\n",
    "field_pdf = norm.pdf(np.log10(di), loc=np.log10(damage), scale=field_std)\n",
    "ax.semilogx(di, field_pdf, label='pdf_load',color = 'r')\n",
    "plt.xlabel(\"Damage\")\n",
    "plt.ylabel(\"pdf\")\n",
    "plt.title(\"Failure probability = %.2e\" %fp.FailureProbability(D50, std).pf_norm_load(damage, field_std))  \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local stress approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FE based failure probability calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_mesh = pylife.vmap.VMAPImport(\"plate_with_hole.vmap\")\n",
    "pyLife_mesh = (vm_mesh.make_mesh('1', 'STATE-2')\n",
    "               .join_coordinates()\n",
    "               .join_variable('STRESS_CAUCHY')\n",
    "               .to_frame())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mises = pyLife_mesh.groupby('element_id')[['S11', 'S22', 'S33', 'S12', 'S13', 'S23']].mean().equistress.mises()\n",
    "mises /= 150.0  # the nominal load level in the FEM analysis\n",
    "#mises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Damage Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_collective = load_dict['total'].load_collective.scale(mises)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage = mat.fatigue.damage(scaled_collective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage = damage.groupby(['element_id']).sum()\n",
    "#damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = pv.UnstructuredGrid(*pyLife_mesh.mesh.vtk_data())\n",
    "plotter = pv.Plotter()\n",
    "plotter.add_mesh(grid, scalars=damage.to_numpy(), log_scale=True, show_edges=True, cmap='jet')\n",
    "plotter.add_scalar_bar()\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximal damage sum: %f\" % damage.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure probability of the plate\n",
    "\n",
    "Often we don't get the volume of the FE data from the result file. But with pyVista we can calculate the volume (or area for 2d elements) easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas =  grid.compute_cell_sizes().cell_data[\"Area\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the failure probability we have to proceed the following steps:\n",
    "\n",
    "* get the failure probality of every element\n",
    "* get the probality of survival for every element\n",
    "* get the probality of survival for the whole component normed based on the volume (or area in 2d) of the element\n",
    "* get the failure probality for the whole component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_per_element = fp.FailureProbability(D50, std).pf_simple_load(damage)\n",
    "probability_of_survival_per_ele = 1 - fp_per_element\n",
    "probability_of_survival_component = (probability_of_survival_per_ele ** (areas/areas.sum())).prod()\n",
    "fp_component = 1 - probability_of_survival_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m' + \"Failure probability of the component is %.2e\" %fp_component)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
